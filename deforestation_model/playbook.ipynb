{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather satellite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pele\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.12).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/akhilchibber/deforestation-detection-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2.00M/10.1G [00:00<1:12:18, 2.50MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m path = \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_download\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43makhilchibber/deforestation-detection-dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPath to dataset files:\u001b[39m\u001b[33m\"\u001b[39m, path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\kagglehub\\datasets.py:35\u001b[39m, in \u001b[36mdataset_download\u001b[39m\u001b[34m(handle, path, force_download)\u001b[39m\n\u001b[32m     33\u001b[39m h = parse_dataset_handle(handle)\n\u001b[32m     34\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading Dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh.to_url()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m, extra={**EXTRA_CONSOLE_BLOCK})\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m path, _ = \u001b[43mregistry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\kagglehub\\registry.py:28\u001b[39m, in \u001b[36mMultiImplRegistry.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m._impls):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m impl.is_supported(*args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     30\u001b[39m         fails.append(\u001b[38;5;28mtype\u001b[39m(impl).\u001b[34m__name__\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\kagglehub\\resolver.py:29\u001b[39m, in \u001b[36mResolver.__call__\u001b[39m\u001b[34m(self, handle, path, force_download)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mself\u001b[39m, handle: T, path: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m, *, force_download: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Optional[\u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m     18\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resolves a handle into a path with the requested file(s) and the resource's version number.\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m \u001b[33;03m        Some cases where version number might be missing: Competition datasource, API-based models.\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     path, version = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Note handles are immutable, so _resolve() could not have altered our reference\u001b[39;00m\n\u001b[32m     32\u001b[39m     register_datasource_access(handle, version)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\kagglehub\\http_resolver.py:130\u001b[39m, in \u001b[36mDatasetHttpResolver._resolve\u001b[39m\u001b[34m(self, h, path, force_download)\u001b[39m\n\u001b[32m    127\u001b[39m os.makedirs(os.path.dirname(archive_path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# First, we download the archive.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[43mapi_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m _extract_archive(archive_path, out_path)\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# Delete the archive\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\kagglehub\\clients.py:217\u001b[39m, in \u001b[36mKaggleApiV1Client.download_file\u001b[39m\u001b[34m(self, path, out_file, resource_handle, cached_path, extract_auto_compressed_file)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownloading from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[43m_download_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_read\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_object\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hash_object:\n\u001b[32m    220\u001b[39m     actual_md5_hash = to_b64_digest(hash_object)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\kagglehub\\clients.py:276\u001b[39m, in \u001b[36m_download_file\u001b[39m\u001b[34m(response, out_file, size_read, total_size, hash_object)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total=total_size, initial=size_read, unit=\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m, unit_scale=\u001b[38;5;28;01mTrue\u001b[39;00m, unit_divisor=\u001b[32m1024\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m progress_bar:\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_file, open_mode) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhash_object\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\requests\\models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\urllib3\\response.py:1066\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1069\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\urllib3\\response.py:955\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    953\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\urllib3\\response.py:879\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    876\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    882\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    888\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    889\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\urllib3\\response.py:862\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    861\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.1008.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"akhilchibber/deforestation-detection-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib numpy scikit-image rasterio tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import normalize\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '/home/epele/.cache/kagglehub/datasets/akhilchibber/deforestation-detection-dataset'\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    print(\"Directory:\", root)\n",
    "    print(\"Subdirectories:\", dirs)\n",
    "    print(\"Files:\", files)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deforestation imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to a sample .tif file\n",
    "tif_file = '/home/epele/.cache/kagglehub/datasets/akhilchibber/deforestation-detection-dataset/versions/1/1_CLOUD_FREE_DATASET/2_SENTINEL2/IMAGE_16_GRID/RASTER_0.tif'\n",
    "\n",
    "# Open and display the image\n",
    "with rasterio.open(tif_file) as src:\n",
    "    image = src.read(1)  # Read the first band\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(\"Sample .tif Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "with rasterio.open('/home/epele/.cache/kagglehub/datasets/akhilchibber/deforestation-detection-dataset/versions/1/1_CLOUD_FREE_DATASET/2_SENTINEL2/IMAGE_16_GRID/RASTER_0.tif') as src:\n",
    "    image = src.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inspect the raw file\n",
    "file_path = '/home/epele/.cache/kagglehub/datasets/akhilchibber/deforestation-detection-dataset/versions/1/1_CLOUD_FREE_DATASET/2_SENTINEL2/IMAGE_16_GRID/RASTER_0.tif'\n",
    "\n",
    "# Load as a memory map to avoid excessive memory usage\n",
    "with open(file_path, 'rb') as f:\n",
    "    raw_data = np.fromfile(f, dtype='float32')  # Adjust dtype based on your file\n",
    "    print(f\"Raw data size: {raw_data.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "array_size = 31724006\n",
    "for i in range(1, int(math.sqrt(array_size)) + 1):\n",
    "    if array_size % i == 0:\n",
    "        print(f\"Possible dimensions: {i} x {array_size // i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bands = array_size // (512 * 512)  # Replace 512 with plausible dimensions\n",
    "print(f\"Number of bands: {num_bands}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import normalize\n",
    "\n",
    "# Paths\n",
    "image_dir = '/home/epele/.cache/kagglehub/datasets/akhilchibber/deforestation-detection-dataset/versions/1/1_CLOUD_FREE_DATASET/2_SENTINEL2/IMAGE_16_GRID/'\n",
    "mask_dir = '/home/epele/.cache/kagglehub/datasets/akhilchibber/deforestation-detection-dataset/versions/1/3_TRAINING_MASKS/MASK_16_GRID/'\n",
    "\n",
    "# Parameters\n",
    "IMG_HEIGHT, IMG_WIDTH = 256, 256  # Fixed size for training\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function\n",
    "def preprocess_image(filepath, target_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "    image = imread(filepath)  # Read image\n",
    "    image = normalize(image, axis=1)  # Normalize to [0, 1]\n",
    "    image = resize(image, target_size, mode='reflect', anti_aliasing=True)  # Resize\n",
    "    return image\n",
    "\n",
    "def preprocess_mask(filepath, target_size=(IMG_HEIGHT, IMG_WIDTH)):\n",
    "    mask = imread(filepath, as_gray=True)  # Read mask (grayscale)\n",
    "    mask = resize(mask, target_size, mode='reflect', anti_aliasing=True)  # Resize\n",
    "    mask = np.round(mask)  # Ensure binary values (0 or 1)\n",
    "    return mask\n",
    "\n",
    "# Load data\n",
    "image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.tif')])\n",
    "mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith('.tif')])\n",
    "\n",
    "images = np.array([preprocess_image(f) for f in image_files])\n",
    "masks = np.array([preprocess_mask(f) for f in mask_files])\n",
    "\n",
    "print(f\"Images shape: {images.shape}, Masks shape: {masks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_and_preprocess_sentinel_image(tif_path, is_mask=False):\n",
    "    \"\"\"\n",
    "    Load and preprocess image/mask\n",
    "    is_mask: boolean to indicate if the file is a mask\n",
    "    \"\"\"\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        if is_mask:\n",
    "            # For masks: load single band and ensure binary values\n",
    "            mask = src.read(1)  # Read first band only\n",
    "            mask = (mask > 0).astype(np.float32)  # Convert to binary (0 and 1)\n",
    "            return mask\n",
    "        else:\n",
    "            # For images: load all 4 bands\n",
    "            image = src.read()\n",
    "            \n",
    "            # Normalize values (based on your data range)\n",
    "            image = image / 6000.0  # Adjust based on your max value (~5794)\n",
    "            \n",
    "            # Transpose to channel-last format (HEIGHT, WIDTH, CHANNELS)\n",
    "            image = np.transpose(image, (1, 2, 0))\n",
    "            \n",
    "            return image\n",
    "\n",
    "def load_dataset(image_dir, mask_dir, image_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Load and preprocess the complete dataset\n",
    "    \"\"\"\n",
    "    # Get file lists\n",
    "    image_files = sorted([os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.tif')])\n",
    "    mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith('.tif')])\n",
    "    \n",
    "    # Lists to store processed data\n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    for img_path, mask_path in zip(image_files, mask_files):\n",
    "        # Load and preprocess image\n",
    "        img = load_and_preprocess_sentinel_image(img_path, is_mask=False)\n",
    "        \n",
    "        # Load and preprocess mask\n",
    "        mask = load_and_preprocess_sentinel_image(mask_path, is_mask=True)\n",
    "        \n",
    "        # Resize if needed\n",
    "        if image_size is not None:\n",
    "            img = tf.image.resize(img, image_size)\n",
    "            mask = tf.image.resize(mask[..., np.newaxis], image_size)\n",
    "        \n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    images = np.array(images)\n",
    "    masks = np.array(masks)\n",
    "    \n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Images shape: {images.shape}\")\n",
    "    print(f\"Masks shape: {masks.shape}\")\n",
    "    \n",
    "    return images, masks\n",
    "\n",
    "# Example usage\n",
    "def prepare_training_data(image_dir, mask_dir, image_size=(256, 256), val_split=0.2):\n",
    "    \"\"\"\n",
    "    Prepare data for training\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    images, masks = load_dataset(image_dir, mask_dir, image_size=image_size)\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        images, masks, test_size=val_split, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(\"\\nData split complete:\")\n",
    "    print(f\"Training images shape: {X_train.shape}\")\n",
    "    print(f\"Training masks shape: {y_train.shape}\")\n",
    "    print(f\"Validation images shape: {X_val.shape}\")\n",
    "    print(f\"Validation masks shape: {y_val.shape}\")\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "# Use the functions\n",
    "image_dir = \"/home/epele/.cache/kagglehub/datasets/akhilchibber/deforestation-detection-dataset/versions/1/1_CLOUD_FREE_DATASET/2_SENTINEL2/IMAGE_16_GRID\"\n",
    "mask_dir = \"/home/epele/.cache/kagglehub/datasets/akhilchibber/deforestation-detection-dataset/versions/1/3_TRAINING_MASKS/MASK_16_GRID\"\n",
    "\n",
    "# Prepare the data\n",
    "X_train, X_val, y_train, y_val = prepare_training_data(\n",
    "    image_dir, \n",
    "    mask_dir,\n",
    "    image_size=(256, 256),  # Resize images to 256x256\n",
    "    val_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout\n",
    "\n",
    "def create_unet(input_shape=(256, 256, 4)):\n",
    "    \"\"\"\n",
    "    Create basic U-Net model\n",
    "    \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    # Bridge\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    \n",
    "    # Decoder\n",
    "    up1 = UpSampling2D(size=(2, 2))(conv3)\n",
    "    up1 = concatenate([conv2, up1], axis=3)\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(up1)\n",
    "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
    "    \n",
    "    up2 = UpSampling2D(size=(2, 2))(conv4)\n",
    "    up2 = concatenate([conv1, up2], axis=3)\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(up2)\n",
    "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
    "    \n",
    "    # Output\n",
    "    outputs = Conv2D(1, 1, activation='sigmoid')(conv5)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = create_unet(input_shape=(256, 256, 4))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.IoU(num_classes=2, target_class_ids=[1])]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=16,\n",
    "    epochs=50,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss during Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy during Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('unet_deforestation_model.h5')\n",
    "\n",
    "# Evaluate\n",
    "results = model.evaluate(X_val, y_val)\n",
    "loss = results[0]\n",
    "accuracy = results[1]\n",
    "print(f\"Validation loss: {loss}, Validation accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation dataset\n",
    "predicted_masks = model.predict(X_val)\n",
    "\n",
    "# Threshold the predictions to get binary masks (0 or 1)\n",
    "threshold = 0.5\n",
    "predicted_masks = (predicted_masks > threshold).astype(np.uint8)\n",
    "\n",
    "print(f\"Predicted masks shape: {predicted_masks.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_sample(image, ground_truth, prediction, index):\n",
    "    \"\"\"\n",
    "    Plot a single image with its ground truth mask and predicted mask.\n",
    "    Args:\n",
    "        image: The original image (input to the model).\n",
    "        ground_truth: The ground truth mask (label).\n",
    "        prediction: The predicted mask from the model.\n",
    "        index: Index of the image to visualize.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Original Image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(image[index])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Ground Truth Mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.imshow(ground_truth[index], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Predicted Mask\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.imshow(prediction[index], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example: Plot the first sample\n",
    "plot_sample(X_val, y_val, predicted_masks, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_samples(images, ground_truths, predictions, num_samples=5):\n",
    "    \"\"\"\n",
    "    Plot multiple images with their ground truth and predicted masks.\n",
    "    Args:\n",
    "        images: Array of input images.\n",
    "        ground_truths: Array of ground truth masks.\n",
    "        predictions: Array of predicted masks.\n",
    "        num_samples: Number of samples to visualize.\n",
    "    \"\"\"\n",
    "    # Use the minimum of num_samples or the number of available samples\n",
    "    num_samples = min(num_samples, len(images))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        plot_sample(images, ground_truths, predictions, index=i)\n",
    "\n",
    "# Plot the first 5 samples (adjusting for available data size)\n",
    "plot_multiple_samples(X_val, y_val, predicted_masks, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\home\\\\epele\\\\.cache\\\\kagglehub\\\\datasets\\\\akhilchibber\\\\deforestation-detection-dataset\\\\versions\\\\1\\\\3_TRAINING_MASKS\\\\MASK_16_GRID\\\\RASTER_0.tif'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_34972\\480537479.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m skimage.io \u001b[38;5;28;01mimport\u001b[39;00m imread\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load a sample image and mask\u001b[39;00m\n\u001b[32m      5\u001b[39m sample_image = \u001b[33m'/home/epele/.cache/kagglehub/datasets/akhilchibber/deforestation-detection-dataset/versions/1/1_CLOUD_FREE_DATASET/2_SENTINEL2/IMAGE_16_GRID/RASTER_0.tif'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m sample_mask = imread(\u001b[33m'/home/epele/.cache/kagglehub/datasets/akhilchibber/deforestation-detection-dataset/versions/1/3_TRAINING_MASKS/MASK_16_GRID/RASTER_0.tif'\u001b[39m)\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Plot sample image and mask\u001b[39;00m\n\u001b[32m      9\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m6\u001b[39m))\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\skimage\\_shared\\utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m                 \u001b[38;5;28;01melif\u001b[39;00m self.new_name \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    325\u001b[39m                     \u001b[38;5;66;03m# Assign old value to new one\u001b[39;00m\n\u001b[32m    326\u001b[39m                     kwargs[self.new_name] = deprecated_value\n\u001b[32m    327\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\skimage\\io\\_io.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(fname, as_gray, plugin, **plugin_args)\u001b[39m\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m fname.lower().endswith((\u001b[33m'.tiff'\u001b[39m, \u001b[33m'.tif'\u001b[39m)):\n\u001b[32m     79\u001b[39m             plugin = \u001b[33m'tifffile'\u001b[39m\n\u001b[32m     80\u001b[39m \n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m file_or_url_context(fname) \u001b[38;5;28;01mas\u001b[39;00m fname, _hide_plugin_deprecation_warnings():\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         img = call_plugin(\u001b[33m'imread'\u001b[39m, fname, plugin=plugin, **plugin_args)\n\u001b[32m     83\u001b[39m \n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m hasattr(img, \u001b[33m'ndim'\u001b[39m):\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\skimage\\_shared\\utils.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    534\u001b[39m         @functools.wraps(func)\n\u001b[32m    535\u001b[39m         \u001b[38;5;28;01mdef\u001b[39;00m wrapped(*args, **kwargs):\n\u001b[32m    536\u001b[39m             stacklevel = \u001b[32m1\u001b[39m + self.get_stack_length(func) - stack_rank\n\u001b[32m    537\u001b[39m             warnings.warn(message, category=FutureWarning, stacklevel=stacklevel)\n\u001b[32m--> \u001b[39m\u001b[32m538\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\skimage\\io\\manage_plugins.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(kind, *args, **kwargs)\u001b[39m\n\u001b[32m    250\u001b[39m             func = [f \u001b[38;5;28;01mfor\u001b[39;00m (p, f) \u001b[38;5;28;01min\u001b[39;00m plugin_funcs \u001b[38;5;28;01mif\u001b[39;00m p == plugin][\u001b[32m0\u001b[39m]\n\u001b[32m    251\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m IndexError:\n\u001b[32m    252\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m RuntimeError(f'Could not find the plugin \"{plugin}\" for {kind}.')\n\u001b[32m    253\u001b[39m \n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\skimage\\io\\_plugins\\tifffile_plugin.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(fname, **kwargs)\u001b[39m\n\u001b[32m     70\u001b[39m     \"\"\"\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'img_num'\u001b[39m \u001b[38;5;28;01min\u001b[39;00m kwargs:\n\u001b[32m     72\u001b[39m         kwargs[\u001b[33m'key'\u001b[39m] = kwargs.pop(\u001b[33m'img_num'\u001b[39m)\n\u001b[32m     73\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tifffile_imread(fname, **kwargs)\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tifffile\\tifffile.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(files, selection, aszarr, key, series, level, squeeze, maxworkers, buffersize, mode, name, offset, size, pattern, axesorder, categories, imread, imreadargs, sort, container, chunkshape, chunkdtype, axestiled, ioworkers, chunkmode, fillvalue, zattrs, multiscales, omexml, out, out_inplace, _multifile, _useframes, **kwargs)\u001b[39m\n\u001b[32m   1224\u001b[39m         ):\n\u001b[32m   1225\u001b[39m             files = files[\u001b[32m0\u001b[39m]\n\u001b[32m   1226\u001b[39m \n\u001b[32m   1227\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m isinstance(files, str) \u001b[38;5;28;01mor\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m isinstance(files, Sequence):\n\u001b[32m-> \u001b[39m\u001b[32m1228\u001b[39m             with TiffFile(\n\u001b[32m   1229\u001b[39m                 files,\n\u001b[32m   1230\u001b[39m                 mode=mode,\n\u001b[32m   1231\u001b[39m                 name=name,\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tifffile\\tifffile.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, file, mode, name, offset, size, omexml, _multifile, _useframes, _parent, **is_flags)\u001b[39m\n\u001b[32m   4252\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m ValueError(\u001b[33m'invalid OME-XML'\u001b[39m)\n\u001b[32m   4253\u001b[39m             self._omexml = omexml\n\u001b[32m   4254\u001b[39m             self.is_ome = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   4255\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m4256\u001b[39m         fh = FileHandle(file, mode=mode, name=name, offset=offset, size=size)\n\u001b[32m   4257\u001b[39m         self._fh = fh\n\u001b[32m   4258\u001b[39m         self._multifile = \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m _multifile \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m bool(_multifile)\n\u001b[32m   4259\u001b[39m         self._files = {fh.name: self}\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tifffile\\tifffile.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, file, mode, name, offset, size)\u001b[39m\n\u001b[32m  14631\u001b[39m         self._offset = -\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m offset \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m offset\n\u001b[32m  14632\u001b[39m         self._size = -\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m size\n\u001b[32m  14633\u001b[39m         self._close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m  14634\u001b[39m         self._lock = NullContext()\n\u001b[32m> \u001b[39m\u001b[32m14635\u001b[39m         self.open()\n\u001b[32m  14636\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m self._fh \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tifffile\\tifffile.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m  14650\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m self._mode \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m {\u001b[33m'rb'\u001b[39m, \u001b[33m'r+b'\u001b[39m, \u001b[33m'wb'\u001b[39m, \u001b[33m'xb'\u001b[39m}:\n\u001b[32m  14651\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f'invalid mode {self._mode}')\n\u001b[32m  14652\u001b[39m             self._file = os.path.realpath(self._file)\n\u001b[32m  14653\u001b[39m             self._dir, self._name = os.path.split(self._file)\n\u001b[32m> \u001b[39m\u001b[32m14654\u001b[39m             self._fh = open(self._file, self._mode, encoding=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m  14655\u001b[39m             self._close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m  14656\u001b[39m             self._offset = max(\u001b[32m0\u001b[39m, self._offset)\n\u001b[32m  14657\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m isinstance(self._file, FileHandle):\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'E:\\\\home\\\\epele\\\\.cache\\\\kagglehub\\\\datasets\\\\akhilchibber\\\\deforestation-detection-dataset\\\\versions\\\\1\\\\3_TRAINING_MASKS\\\\MASK_16_GRID\\\\RASTER_0.tif'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "\n",
    "# Load a sample image and mask\n",
    "sample_image = '/home/epele/.cache/kagglehub/datasets/akhilchibber/deforestation-detection-dataset/versions/1/1_CLOUD_FREE_DATASET/2_SENTINEL2/IMAGE_16_GRID/RASTER_0.tif'\n",
    "sample_mask = imread('/home/epele/.cache/kagglehub/datasets/akhilchibber/deforestation-detection-dataset/versions/1/3_TRAINING_MASKS/MASK_16_GRID/RASTER_0.tif')\n",
    "\n",
    "# Plot sample image and mask\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "with rasterio.open(sample_image) as src:\n",
    "    image = src.read(1)  # Read the first band\n",
    "    plt.imshow(image, cmap='gist_earth')\n",
    "    plt.title(\"Sample Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Sample Mask\")\n",
    "plt.imshow(sample_mask, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/home/epele/.cache/kagglehub/datasets/akhilchibber/deforestation-detection-dataset/versions/1/1_CLOUD_FREE_DATASET/2_SENTINEL2/IMAGE_FULL/raster.tif'\n",
    "sample_image = imread(image_path)\n",
    "print(f\"Image shape: {sample_image.shape}\")\n",
    "\n",
    "with rasterio.open(image_path) as src:\n",
    "    image = src.read(1)  # Read the first band\n",
    "    plt.imshow(image, cmap='gist_earth')\n",
    "    plt.title(\"Sample .tif Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definim punctele unui cub n spaiul 3D.\n",
    "# Cubul are o fa la z = 3 i cealalt la z = 5.\n",
    "points = np.array([\n",
    "    [-1, -1, 3],\n",
    "    [ 1, -1, 3],\n",
    "    [ 1,  1, 3],\n",
    "    [-1,  1, 3],\n",
    "    [-1, -1, 5],\n",
    "    [ 1, -1, 5],\n",
    "    [ 1,  1, 5],\n",
    "    [-1,  1, 5]\n",
    "])\n",
    "\n",
    "# Funcia de proiecie simpl: aplic formula x' = (d*x)/z, y' = (d*y)/z.\n",
    "def project(point, d=1):\n",
    "    x, y, z = point\n",
    "    return np.array([d * x / z, d * y / z])\n",
    "\n",
    "# Proiectm toate punctele cubului.\n",
    "projected_points = np.array([project(p, d=1) for p in points])\n",
    "\n",
    "# Configurm graficul.\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(projected_points[:,0], projected_points[:,1], color='blue')\n",
    "for i, p in enumerate(projected_points):\n",
    "    plt.text(p[0], p[1], str(i), color='red', fontsize=12)\n",
    "\n",
    "# Definim muchiile cubului pentru a-l desena corect.\n",
    "edges = [\n",
    "    (0,1), (1,2), (2,3), (3,0),  # faa din fa\n",
    "    (4,5), (5,6), (6,7), (7,4),  # faa din spate\n",
    "    (0,4), (1,5), (2,6), (3,7)   # muchiile laterale\n",
    "]\n",
    "\n",
    "# Desenm muchiile cubului.\n",
    "for e in edges:\n",
    "    p1 = projected_points[e[0]]\n",
    "    p2 = projected_points[e[1]]\n",
    "    plt.plot([p1[0], p2[0]], [p1[1], p2[1]], 'k-')\n",
    "\n",
    "plt.xlabel(\"x'\")\n",
    "plt.ylabel(\"y'\")\n",
    "plt.title(\"Proiecie perspectiv a unui cub 3D pe planul 2D\")\n",
    "plt.grid(True)\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
